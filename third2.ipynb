{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd13707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Initial training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sameer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sameer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Train acc 0.218, Test acc 0.247\n",
      "Epoch 2/20: Train acc 0.283, Test acc 0.323\n",
      "Epoch 3/20: Train acc 0.310, Test acc 0.346\n",
      "Epoch 4/20: Train acc 0.361, Test acc 0.412\n",
      "Epoch 5/20: Train acc 0.391, Test acc 0.357\n",
      "Epoch 6/20: Train acc 0.392, Test acc 0.449\n",
      "Epoch 7/20: Train acc 0.426, Test acc 0.404\n",
      "Epoch 8/20: Train acc 0.436, Test acc 0.433\n",
      "Epoch 9/20: Train acc 0.469, Test acc 0.437\n",
      "Epoch 10/20: Train acc 0.481, Test acc 0.451\n",
      "Epoch 11/20: Train acc 0.529, Test acc 0.495\n",
      "Epoch 12/20: Train acc 0.544, Test acc 0.507\n",
      "Epoch 13/20: Train acc 0.557, Test acc 0.513\n",
      "Epoch 14/20: Train acc 0.557, Test acc 0.518\n",
      "Epoch 15/20: Train acc 0.573, Test acc 0.521\n",
      "Epoch 16/20: Train acc 0.576, Test acc 0.527\n",
      "Epoch 17/20: Train acc 0.577, Test acc 0.528\n",
      "Epoch 18/20: Train acc 0.587, Test acc 0.524\n",
      "Epoch 19/20: Train acc 0.588, Test acc 0.530\n",
      "Epoch 20/20: Train acc 0.589, Test acc 0.535\n",
      "[2] Computing Fisher on forget set...\n",
      "Applying 5 SSD steps...\n",
      "Fine-tuning retained classes...\n",
      " Fine-tune acc: 0.566\n",
      " Fine-tune acc: 0.581\n",
      " Fine-tune acc: 0.596\n",
      "Post-unlearn test acc: 0.421\n",
      "[3] Incompetent Teacher unlearning...\n",
      "Epoch 1/20 - Student training loss: 187.2352\n",
      "Epoch 2/20 - Student training loss: 109.8082\n",
      "Epoch 3/20 - Student training loss: 80.8203\n",
      "Epoch 4/20 - Student training loss: 70.6643\n",
      "Epoch 5/20 - Student training loss: 63.4659\n",
      "Epoch 6/20 - Student training loss: 57.4404\n",
      "Epoch 7/20 - Student training loss: 53.4767\n",
      "Epoch 8/20 - Student training loss: 50.7514\n",
      "Epoch 9/20 - Student training loss: 51.1117\n",
      "Epoch 10/20 - Student training loss: 50.9511\n",
      "Epoch 11/20 - Student training loss: 47.6830\n",
      "Epoch 12/20 - Student training loss: 45.6257\n",
      "Epoch 13/20 - Student training loss: 46.4690\n",
      "Epoch 14/20 - Student training loss: 45.5549\n",
      "Epoch 15/20 - Student training loss: 43.7249\n",
      "Epoch 16/20 - Student training loss: 42.9341\n",
      "Epoch 17/20 - Student training loss: 41.4516\n",
      "Epoch 18/20 - Student training loss: 43.6550\n",
      "Epoch 19/20 - Student training loss: 41.7415\n",
      "Epoch 20/20 - Student training loss: 41.6736\n",
      "Student model test acc after distillation: 0.484\n",
      "\n",
      "=== Summary ===\n",
      "Pre-unlearning model   test acc: 0.535\n",
      "Post-unlearning model  test acc: 0.421\n",
      "Retrained baseline model test acc: 0.448\n",
      "Student (Incompetent Teacher) model test acc: 0.484\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "# ====================== Configuration ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "learning_rate = 0.01        # initial LR\n",
    "num_epochs = 20             # training epochs\n",
    "ssd_lambda = 1e-4           # SSD strength\n",
    "forget_classes = [0, 1]     # classes to unlearn\n",
    "\n",
    "# Paths for checkpoints\n",
    "pre_unlearn_path = 'resnet18_pre_unlearn.pth'\n",
    "post_unlearn_path = 'resnet18_post_unlearn.pth'\n",
    "baseline_path     = 'resnet18_baseline.pth'\n",
    "teacher_path = 'resnet18_teacher.pth'\n",
    "\n",
    "# ====================== Data Preparation ======================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "full_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "full_test  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Subsample speed-up: 5000 train, 1000 test\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.permutation(len(full_train))[:5000]\n",
    "test_idx  = np.random.permutation(len(full_test))[:1000]\n",
    "train_dataset = Subset(full_train, train_idx)\n",
    "test_dataset  = Subset(full_test,  test_idx)\n",
    "train_loader   = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader    = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Forget and retain subsets from sampled train\n",
    "forget_idx = [i for i,(_,t) in enumerate(train_dataset) if t in forget_classes]\n",
    "retain_idx = [i for i,(_,t) in enumerate(train_dataset) if t not in forget_classes]\n",
    "forget_loader = DataLoader(Subset(train_dataset, forget_idx), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "retain_loader = DataLoader(Subset(train_dataset, retain_idx), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# ====================== Model & Utilities ======================\n",
    "\n",
    "def get_model():\n",
    "    m = models.resnet18(pretrained=False, num_classes=10).to(device)\n",
    "    return m\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train/eval routines\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = correct = total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = out.argmax(1)\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = out.argmax(1)\n",
    "            correct += preds.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# ====================== 1) Initial Training ======================\n",
    "model = get_model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs // 2, gamma=0.1)\n",
    "print(\"[1] Initial training...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: Train acc {train_acc:.3f}, Test acc {test_acc:.3f}\")\n",
    "# save pre-unlearn\n",
    "torch.save(model.state_dict(), pre_unlearn_path)\n",
    "\n",
    "# ====================== 2) SSD Unlearning ======================\n",
    "# compute Fisher\n",
    "print(\"[2] Computing Fisher on forget set...\")\n",
    "fisher = {n: torch.zeros_like(p) for n, p in model.named_parameters() if p.requires_grad}\n",
    "samples = 0\n",
    "model.eval()\n",
    "for x, y in forget_loader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    model.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    samples += 1\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad and p.grad is not None:\n",
    "            fisher[n] += p.grad.data.pow(2)\n",
    "fisher = {n: v / samples for n, v in fisher.items()}\n",
    "# apply multi-step SSD\n",
    "grain_steps = 5\n",
    "print(f\"Applying {grain_steps} SSD steps...\")\n",
    "for _ in range(grain_steps):\n",
    "    with torch.no_grad():\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                p.sub_(ssd_lambda * fisher[n] * p)\n",
    "# fine-tune on retain set\n",
    "opt2 = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "print(\"Fine-tuning retained classes...\")\n",
    "for _ in range(3):\n",
    "    _, acc = train_one_epoch(model, retain_loader, opt2)\n",
    "    print(f\" Fine-tune acc: {acc:.3f}\")\n",
    "# save post-unlearn\n",
    "torch.save(model.state_dict(), post_unlearn_path)\n",
    "post_loss, post_acc = evaluate(model, test_loader)\n",
    "print(f\"Post-unlearn test acc: {post_acc:.3f}\")\n",
    "\n",
    "# ====================== 3) Incompetent Teacher Unlearning ======================\n",
    "# Load Teacher Model (Pre-unlearned model)\n",
    "teacher_model = get_model()\n",
    "teacher_model.load_state_dict(torch.load(pre_unlearn_path))\n",
    "teacher_model.eval()\n",
    "print(\"[3] Incompetent Teacher unlearning...\")\n",
    "\n",
    "# Define student model (to be trained with distillation)\n",
    "student_model = get_model()\n",
    "optimizer_student = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "def distillation_loss(student_output, teacher_output, temperature=3.0, alpha=0.7):\n",
    "    soft_target = nn.functional.softmax(teacher_output / temperature, dim=1)\n",
    "    hard_target = nn.functional.softmax(student_output / temperature, dim=1)\n",
    "    return nn.KLDivLoss(reduction='batchmean')(hard_target.log(), soft_target) * (temperature ** 2) * alpha + nn.CrossEntropyLoss()(student_output, hard_target.argmax(1)) * (1. - alpha)\n",
    "\n",
    "# Train student model on retained data\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    student_model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in retain_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer_student.zero_grad()\n",
    "        student_out = student_model(x)\n",
    "        teacher_out = teacher_model(x)\n",
    "        loss = distillation_loss(student_out, teacher_out)\n",
    "        loss.backward()\n",
    "        optimizer_student.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} - Student training loss: {total_loss / len(retain_loader):.4f}\")\n",
    "\n",
    "# Save the student model\n",
    "torch.save(student_model.state_dict(), teacher_path)\n",
    "student_loss, student_acc = evaluate(student_model, test_loader)\n",
    "print(f\"Student model test acc after distillation: {student_acc:.3f}\")\n",
    "\n",
    "# ====================== 4) Summary Comparison ======================\n",
    "print(\"\\n=== Summary ===\")\n",
    "# reload to ensure fairness\n",
    "t1 = get_model(); t1.load_state_dict(torch.load(pre_unlearn_path)); _, pre_acc = evaluate(t1, test_loader)\n",
    "print(f\"Pre-unlearning model   test acc: {pre_acc:.3f}\")\n",
    "t2 = get_model(); t2.load_state_dict(torch.load(post_unlearn_path)); _, post_acc = evaluate(t2, test_loader)\n",
    "print(f\"Post-unlearning model  test acc: {post_acc:.3f}\")\n",
    "t3 = get_model(); t3.load_state_dict(torch.load(baseline_path)); _, base_acc = evaluate(t3, test_loader)\n",
    "print(f\"Retrained baseline model test acc: {base_acc:.3f}\")\n",
    "t4 = get_model(); t4.load_state_dict(torch.load(teacher_path)); _, student_acc = evaluate(t4, test_loader)\n",
    "print(f\"Student (Incompetent Teacher) model test acc: {student_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3679ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
