{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84949964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Initial training on FULL dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sameer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sameer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Train acc 0.389, Test acc 0.510\n",
      "Epoch 2/20: Train acc 0.510, Test acc 0.571\n",
      "Epoch 3/20: Train acc 0.578, Test acc 0.632\n",
      "Epoch 4/20: Train acc 0.630, Test acc 0.644\n",
      "Epoch 5/20: Train acc 0.658, Test acc 0.682\n",
      "Epoch 6/20: Train acc 0.681, Test acc 0.693\n",
      "Epoch 7/20: Train acc 0.700, Test acc 0.712\n",
      "Epoch 8/20: Train acc 0.715, Test acc 0.721\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[1] Initial training on FULL dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 93\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[1;32m     95\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[1], line 64\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer)\u001b[0m\n\u001b[1;32m     62\u001b[0m out \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     63\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y)\n\u001b[0;32m---> 64\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     66\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ====================== Configuration ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "learning_rate = 0.01        # initial LR\n",
    "num_epochs = 20             # training epochs\n",
    "ssd_lambda = 1e-4           # SSD strength\n",
    "forget_classes = [0, 1]     # classes to unlearn\n",
    "\n",
    "# Paths for checkpoints\n",
    "pre_unlearn_path = 'resnet18_pre_unlearn.pth'\n",
    "post_unlearn_path = 'resnet18_post_unlearn.pth'\n",
    "baseline_path     = 'resnet18_baseline.pth'\n",
    "\n",
    "# ====================== Data Preparation ======================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "# Full CIFAR-10 datasets\n",
    "full_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "full_test  = datasets.CIFAR10(root='./data', train=False,download=True, transform=transform_test)\n",
    "\n",
    "test_loader  = DataLoader(full_test,  batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "train_loader = DataLoader(full_train, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "\n",
    "# Forget/retain subsets from full training set\n",
    "forget_idx = [i for i, (_, t) in enumerate(full_train) if t in forget_classes]\n",
    "retain_idx = [i for i, (_, t) in enumerate(full_train) if t not in forget_classes]\n",
    "forget_loader = DataLoader(Subset(full_train, forget_idx), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "retain_loader = DataLoader(Subset(full_train, retain_idx), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# ====================== Model & Utilities ======================\n",
    "\n",
    "def get_model():\n",
    "    return models.resnet18(pretrained=False, num_classes=10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train/eval routines\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = correct = total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = out.argmax(1)\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = out.argmax(1)\n",
    "            correct += preds.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# ====================== 1) Initial Training ======================\n",
    "model = get_model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs//2, gamma=0.1)\n",
    "print(\"[1] Initial training on FULL dataset...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: Train acc {train_acc:.3f}, Test acc {test_acc:.3f}\")\n",
    "# save pre-unlearn\n",
    "torch.save(model.state_dict(), pre_unlearn_path)\n",
    "print(f\"Saved pre-unlearn model to {pre_unlearn_path}\")\n",
    "\n",
    "# ====================== 2) SSD Unlearning (timed) ======================\n",
    "print(\"[2] Starting SSD unlearning pipeline...\")\n",
    "start_ssd = time.time()\n",
    "# compute Fisher\n",
    "fisher = {n: torch.zeros_like(p) for n, p in model.named_parameters() if p.requires_grad}\n",
    "samples = 0\n",
    "model.eval()\n",
    "for x, y in forget_loader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    model.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    samples += 1\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad and p.grad is not None:\n",
    "            fisher[n] += p.grad.data.pow(2)\n",
    "fisher = {n: v / samples for n, v in fisher.items()}\n",
    "# apply SSD + fine-tune\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                p.sub_(ssd_lambda * fisher[n] * p)\n",
    "# fine-tune on retained classes\n",
    "opt2 = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "for _ in range(3):\n",
    "    train_one_epoch(model, retain_loader, opt2)\n",
    "# save post-unlearn\n",
    "torch.save(model.state_dict(), post_unlearn_path)\n",
    "post_loss, post_acc = evaluate(model, test_loader)\n",
    "ssd_time = time.time() - start_ssd\n",
    "print(f\"SSD pipeline done in {ssd_time:.1f}s | Post-unlearn test acc: {post_acc:.3f}\")\n",
    "\n",
    "# ====================== 3) Baseline Retraining (timed) ======================\n",
    "print(\"[3] Retraining baseline on retained data...\")\n",
    "start_retrain = time.time()\n",
    "baseline = get_model()\n",
    "opt3 = optim.SGD(baseline.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "sched3 = optim.lr_scheduler.StepLR(opt3, step_size=num_epochs//2, gamma=0.1)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_one_epoch(baseline, retain_loader, opt3)\n",
    "    sched3.step()\n",
    "baseline_time = time.time() - start_retrain\n",
    "# save baseline\n",
    "torch.save(baseline.state_dict(), baseline_path)\n",
    "base_loss, base_acc = evaluate(baseline, test_loader)\n",
    "print(f\"Baseline retrain done in {baseline_time:.1f}s | Test acc: {base_acc:.3f}\")\n",
    "\n",
    "# ====================== 4) Summary ======================\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Pre-unlearn model       acc: {evaluate(get_model().load_state_dict(torch.load(pre_unlearn_path)), test_loader)[1]:.3f}\")\n",
    "print(f\"Post-unlearn model      acc: {post_acc:.3f}\")\n",
    "print(f\"Baseline retrain model  acc: {base_acc:.3f}\")\n",
    "print(f\"SSD unlearning time     : {ssd_time:.1f}s\")\n",
    "print(f\"Baseline retrain time   : {baseline_time:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a491d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
