{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717716d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sameer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sameer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epochs | init LR=0.01\n",
      "Epoch 1/20: Train loss 2.2486, acc 0.2008 | Test loss 2.3602, acc 0.2140\n",
      " LR now: 0.01000\n",
      "Epoch 2/20: Train loss 2.0202, acc 0.2760 | Test loss 1.9485, acc 0.3140\n",
      " LR now: 0.01000\n",
      "Epoch 3/20: Train loss 1.8255, acc 0.3370 | Test loss 1.8377, acc 0.3340\n",
      " LR now: 0.01000\n",
      "Epoch 4/20: Train loss 1.8017, acc 0.3474 | Test loss 1.7562, acc 0.3820\n",
      " LR now: 0.01000\n",
      "Epoch 5/20: Train loss 1.6966, acc 0.3894 | Test loss 1.6347, acc 0.3970\n",
      " LR now: 0.01000\n",
      "Epoch 6/20: Train loss 1.6176, acc 0.4098 | Test loss 1.5772, acc 0.4330\n",
      " LR now: 0.01000\n",
      "Epoch 7/20: Train loss 1.5467, acc 0.4324 | Test loss 1.6350, acc 0.4230\n",
      " LR now: 0.01000\n",
      "Epoch 8/20: Train loss 1.5151, acc 0.4362 | Test loss 1.5792, acc 0.4220\n",
      " LR now: 0.01000\n",
      "Epoch 9/20: Train loss 1.4976, acc 0.4486 | Test loss 1.7135, acc 0.4390\n",
      " LR now: 0.01000\n",
      "Epoch 10/20: Train loss 1.4775, acc 0.4658 | Test loss 1.5977, acc 0.4350\n",
      " LR now: 0.00100\n",
      "Epoch 11/20: Train loss 1.3313, acc 0.5268 | Test loss 1.3829, acc 0.5100\n",
      " LR now: 0.00100\n",
      "Epoch 12/20: Train loss 1.2411, acc 0.5504 | Test loss 1.3662, acc 0.5230\n",
      " LR now: 0.00100\n",
      "Epoch 13/20: Train loss 1.2138, acc 0.5628 | Test loss 1.3607, acc 0.5080\n",
      " LR now: 0.00100\n",
      "Epoch 14/20: Train loss 1.2036, acc 0.5674 | Test loss 1.3610, acc 0.5240\n",
      " LR now: 0.00100\n",
      "Epoch 15/20: Train loss 1.1846, acc 0.5672 | Test loss 1.3630, acc 0.5170\n",
      " LR now: 0.00100\n",
      "Epoch 16/20: Train loss 1.1794, acc 0.5690 | Test loss 1.3532, acc 0.5330\n",
      " LR now: 0.00100\n",
      "Epoch 17/20: Train loss 1.1766, acc 0.5644 | Test loss 1.3441, acc 0.5370\n",
      " LR now: 0.00100\n",
      "Epoch 18/20: Train loss 1.1450, acc 0.5848 | Test loss 1.3350, acc 0.5470\n",
      " LR now: 0.00100\n",
      "Epoch 19/20: Train loss 1.1516, acc 0.5872 | Test loss 1.3395, acc 0.5410\n",
      " LR now: 0.00100\n",
      "Epoch 20/20: Train loss 1.1499, acc 0.5834 | Test loss 1.3359, acc 0.5240\n",
      " LR now: 0.00010\n",
      "Computing Fisher on forget set...\n",
      "Performing 5 SSD steps at λ=0.0001\n",
      "SSD step applied.\n",
      "SSD step applied.\n",
      "SSD step applied.\n",
      "SSD step applied.\n",
      "SSD step applied.\n",
      "Fine-tuning on retained classes...\n",
      " Fine-tune epoch 1: loss 1.2154, acc 0.5601\n",
      " Fine-tune epoch 2: loss 1.1442, acc 0.5893\n",
      " Fine-tune epoch 3: loss 1.1123, acc 0.6019\n",
      "Final Test loss: 1.4148, acc: 0.4950\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "# ====================== Configuration ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "learning_rate = 0.01        # reduced initial LR for stability\n",
    "num_epochs = 20             # extend training for better convergence\n",
    "ssd_lambda = 1e-4           # smaller dampening strength\n",
    "forget_classes = [0, 1]     # CIFAR-10 classes to forget\n",
    "\n",
    "# ====================== Data Preparation ======================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "full_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "full_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Subsample 5000 train and 1000 test images for speed\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.permutation(len(full_train))[:5000]\n",
    "test_idx  = np.random.permutation(len(full_test))[:1000]\n",
    "train_dataset = Subset(full_train, train_idx)\n",
    "test_dataset  = Subset(full_test,  test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Create forget subset\n",
    "forget_indices = [i for i, (_, t) in enumerate(train_dataset) if t in forget_classes]\n",
    "forget_loader  = DataLoader(Subset(train_dataset, forget_indices), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# ====================== Model, Loss, Optimizer ======================\n",
    "model = models.resnet18(pretrained=False, num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "# LR scheduler to decay LR midway\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs//2, gamma=0.1)\n",
    "\n",
    "# ====================== Train & Eval Functions ======================\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = correct = total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = out.argmax(1)\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = out.argmax(1)\n",
    "            correct += preds.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "# ====================== Initial Training ======================\n",
    "print(f\"Training for {num_epochs} epochs | init LR={learning_rate}\")\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    test_loss, test_acc   = evaluate(model, test_loader)\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "          f\"Train loss {train_loss:.4f}, acc {train_acc:.4f} | \"\n",
    "          f\"Test loss {test_loss:.4f}, acc {test_acc:.4f}\")\n",
    "    scheduler.step()\n",
    "    print(f\" LR now: {scheduler.get_last_lr()[0]:.5f}\")\n",
    "\n",
    "# ====================== Compute Fisher Information ======================\n",
    "def compute_fisher(model, loader):\n",
    "    model.eval()\n",
    "    fisher = {n: torch.zeros_like(p) for n, p in model.named_parameters() if p.requires_grad}\n",
    "    samples = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        model.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        samples += 1\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad and p.grad is not None:\n",
    "                fisher[n] += p.grad.data.clone().pow(2)\n",
    "    return {n: v/samples for n, v in fisher.items()}\n",
    "\n",
    "print(\"Computing Fisher on forget set...\")\n",
    "fisher_info = compute_fisher(model, forget_loader)\n",
    "\n",
    "# ====================== Selective Synaptic Dampening ======================\n",
    "def selective_synaptic_dampening(model, fisher_info, lmbda):\n",
    "    with torch.no_grad():\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                p.sub_(lmbda * fisher_info[n] * p)\n",
    "    print(\"SSD step applied.\")\n",
    "\n",
    "# Multi-step SSD and fine-tuning\n",
    "grain_steps = 5\n",
    "print(f\"Performing {grain_steps} SSD steps at λ={ssd_lambda}\")\n",
    "for _ in range(grain_steps):\n",
    "    selective_synaptic_dampening(model, fisher_info, ssd_lambda)\n",
    "\n",
    "# ====================== Post-Unlearning Fine-Tuning ======================\n",
    "retain_idx = [i for i, (_, t) in enumerate(train_dataset) if t not in forget_classes]\n",
    "retain_loader = DataLoader(Subset(train_dataset, retain_idx), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "print(\"Fine-tuning on retained classes...\")\n",
    "for ft in range(3):\n",
    "    loss_ft, acc_ft = train_one_epoch(model, retain_loader)\n",
    "    print(f\" Fine-tune epoch {ft+1}: loss {loss_ft:.4f}, acc {acc_ft:.4f}\")\n",
    "\n",
    "# ====================== Final Evaluation ======================\n",
    "final_loss, final_acc = evaluate(model, test_loader)\n",
    "print(f\"Final Test loss: {final_loss:.4f}, acc: {final_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1b645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
