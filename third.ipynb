{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d34ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Initial training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sameer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sameer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Train acc 0.204, Test acc 0.217\n",
      "Epoch 2/20: Train acc 0.288, Test acc 0.318\n",
      "Epoch 3/20: Train acc 0.337, Test acc 0.348\n",
      "Epoch 4/20: Train acc 0.360, Test acc 0.392\n",
      "Epoch 5/20: Train acc 0.384, Test acc 0.384\n",
      "Epoch 6/20: Train acc 0.420, Test acc 0.421\n",
      "Epoch 7/20: Train acc 0.427, Test acc 0.412\n",
      "Epoch 8/20: Train acc 0.440, Test acc 0.432\n",
      "Epoch 9/20: Train acc 0.469, Test acc 0.407\n",
      "Epoch 10/20: Train acc 0.475, Test acc 0.462\n",
      "Epoch 11/20: Train acc 0.537, Test acc 0.520\n",
      "Epoch 12/20: Train acc 0.555, Test acc 0.541\n",
      "Epoch 13/20: Train acc 0.551, Test acc 0.525\n",
      "Epoch 14/20: Train acc 0.560, Test acc 0.548\n",
      "Epoch 15/20: Train acc 0.566, Test acc 0.529\n",
      "Epoch 16/20: Train acc 0.568, Test acc 0.534\n",
      "Epoch 17/20: Train acc 0.575, Test acc 0.552\n",
      "Epoch 18/20: Train acc 0.578, Test acc 0.542\n",
      "Epoch 19/20: Train acc 0.578, Test acc 0.548\n",
      "Epoch 20/20: Train acc 0.582, Test acc 0.553\n",
      "[2] Computing Fisher on forget set...\n",
      "Applying 5 SSD steps...\n",
      "Fine-tuning retained classes...\n",
      " Fine-tune acc: 0.577\n",
      " Fine-tune acc: 0.566\n",
      " Fine-tune acc: 0.589\n",
      "Post-unlearn test acc: 0.427\n",
      "[3] Baseline training on retained classes...\n",
      " Epoch 5: Re-trained acc 0.455\n",
      " Epoch 10: Re-trained acc 0.550\n",
      " Epoch 15: Re-trained acc 0.633\n",
      " Epoch 20: Re-trained acc 0.666\n",
      "Baseline test acc: 0.448\n",
      "\n",
      "=== Summary ===\n",
      "Pre-unlearning model   test acc: 0.553\n",
      "Post-unlearning model  test acc: 0.427\n",
      "Retrained baseline model test acc: 0.448\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "# ====================== Configuration ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "learning_rate = 0.01        # initial LR\n",
    "num_epochs = 20             # training epochs\n",
    "ssd_lambda = 1e-4           # SSD strength\n",
    "forget_classes = [0, 1]     # classes to unlearn\n",
    "\n",
    "# Paths for checkpoints\n",
    "pre_unlearn_path = 'resnet18_pre_unlearn.pth'\n",
    "post_unlearn_path = 'resnet18_post_unlearn.pth'\n",
    "baseline_path     = 'resnet18_baseline.pth'\n",
    "\n",
    "# ====================== Data Preparation ======================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "full_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "full_test  = datasets.CIFAR10(root='./data', train=False,download=True, transform=transform_test)\n",
    "\n",
    "# Subsample speed-up: 5000 train, 1000 test\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.permutation(len(full_train))[:5000]\n",
    "test_idx  = np.random.permutation(len(full_test))[:1000]\n",
    "train_dataset = Subset(full_train, train_idx)\n",
    "test_dataset  = Subset(full_test,  test_idx)\n",
    "train_loader   = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader    = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Forget and retain subsets from sampled train\n",
    "forget_idx = [i for i,(_,t) in enumerate(train_dataset) if t in forget_classes]\n",
    "retain_idx = [i for i,(_,t) in enumerate(train_dataset) if t not in forget_classes]\n",
    "forget_loader = DataLoader(Subset(train_dataset, forget_idx), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "retain_loader = DataLoader(Subset(train_dataset, retain_idx), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# ====================== Model & Utilities ======================\n",
    "\n",
    "def get_model():\n",
    "    m = models.resnet18(pretrained=False, num_classes=10).to(device)\n",
    "    return m\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train/eval routines\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = correct = total = 0\n",
    "    for x,y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()*x.size(0)\n",
    "        preds = out.argmax(1)\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out,y)\n",
    "            total_loss += loss.item()*x.size(0)\n",
    "            preds = out.argmax(1)\n",
    "            correct += preds.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "# ====================== 1) Initial Training ======================\n",
    "model = get_model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs//2, gamma=0.1)\n",
    "print(\"[1] Initial training...\")\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
    "    test_loss, test_acc   = evaluate(model, test_loader)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: Train acc {train_acc:.3f}, Test acc {test_acc:.3f}\")\n",
    "# save pre-unlearn\n",
    "torch.save(model.state_dict(), pre_unlearn_path)\n",
    "\n",
    "# ====================== 2) SSD Unlearning ======================\n",
    "# compute Fisher\n",
    "print(\"[2] Computing Fisher on forget set...\")\n",
    "fisher = {n: torch.zeros_like(p) for n,p in model.named_parameters() if p.requires_grad}\n",
    "samples=0\n",
    "model.eval()\n",
    "for x,y in forget_loader:\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    model.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = criterion(out,y)\n",
    "    loss.backward()\n",
    "    samples+=1\n",
    "    for n,p in model.named_parameters():\n",
    "        if p.requires_grad and p.grad is not None:\n",
    "            fisher[n]+=p.grad.data.pow(2)\n",
    "fisher = {n: v/samples for n,v in fisher.items()}\n",
    "# apply multi-step SSD\n",
    "grain_steps=5\n",
    "print(f\"Applying {grain_steps} SSD steps...\")\n",
    "for _ in range(grain_steps):\n",
    "    with torch.no_grad():\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                p.sub_(ssd_lambda * fisher[n] * p)\n",
    "# fine-tune on retain set\n",
    "opt2 = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "print(\"Fine-tuning retained classes...\")\n",
    "for _ in range(3):\n",
    "    _,acc = train_one_epoch(model, retain_loader, opt2)\n",
    "    print(f\" Fine-tune acc: {acc:.3f}\")\n",
    "# save post-unlearn\n",
    "torch.save(model.state_dict(), post_unlearn_path)\n",
    "post_loss, post_acc = evaluate(model, test_loader)\n",
    "print(f\"Post-unlearn test acc: {post_acc:.3f}\")\n",
    "\n",
    "# ====================== 3) Baseline Training (no forget data) ======================\n",
    "baseline = get_model()\n",
    "opt3 = optim.SGD(baseline.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "sched3 = optim.lr_scheduler.StepLR(opt3, step_size=num_epochs//2, gamma=0.1)\n",
    "print(\"[3] Baseline training on retained classes...\")\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    _,acc = train_one_epoch(baseline, retain_loader, opt3)\n",
    "    sched3.step()\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\" Epoch {epoch}: Re-trained acc {acc:.3f}\")\n",
    "torch.save(baseline.state_dict(), baseline_path)\n",
    "base_loss, base_acc = evaluate(baseline, test_loader)\n",
    "print(f\"Baseline test acc: {base_acc:.3f}\")\n",
    "\n",
    "# ====================== 4) Summary Comparison ======================\n",
    "print(\"\\n=== Summary ===\")\n",
    "# reload to ensure fairness\n",
    "t1 = get_model(); t1.load_state_dict(torch.load(pre_unlearn_path)); _,pre_acc = evaluate(t1, test_loader)\n",
    "print(f\"Pre-unlearning model   test acc: {pre_acc:.3f}\")\n",
    "t2 = get_model(); t2.load_state_dict(torch.load(post_unlearn_path)); _,post_acc = evaluate(t2, test_loader)\n",
    "print(f\"Post-unlearning model  test acc: {post_acc:.3f}\")\n",
    "t3 = get_model(); t3.load_state_dict(torch.load(baseline_path)); _,base_acc = evaluate(t3, test_loader)\n",
    "print(f\"Retrained baseline model test acc: {base_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b0a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
